

Заголовки HTTP ([HTTP headers](https://ru.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B8%D1%81%D0%BE%D0%BA_%D0%B7%D0%B0%D0%B3%D0%BE%D0%BB%D0%BE%D0%B2%D0%BA%D0%BE%D0%B2_HTTP#%D0%97%D0%B0%D0%B3%D0%BE%D0%BB%D0%BE%D0%B2%D0%BA%D0%B8_%D0%B7%D0%B0%D0%BF%D1%80%D0%BE%D1%81%D0%B0)) являются ключевым компонентом взаимодействия между клиентом и сервером в рамках протокола HTTP ([HyperText Transfer Protocol](https://ru.wikipedia.org/wiki/HTTP)). Они представляют собой набор метаданных, которые сопровождают HTTP-запросы и ответы. Эти метаданные помогают клиенту и серверу правильно интерпретировать передаваемые данные, управлять состоянием сессии, обеспечивать безопасность и многое другое.

При разработке парсеров рекомендуется отправлять свои заголовки вместе с GET-запросом. Это нужно потому, что сервер определяет, что делать с вашим запросом, именно на основе заголовков, которые вы отправили. Иногда требуется маскироваться под браузер, подменять **cookie**, **User-Agent**, **headers**, **IP-адрес** и прочее.

### Основные типы заголовков HTTP:

1. **Общие заголовки (General Headers)**: Эти заголовки присутствуют и в запросах, и в ответах, но не относятся непосредственно к телу сообщения. Пример: `Cache-Control`, `Connection`.
    
2. **Заголовки запроса (Request Headers)**: Отправляются только в HTTP-запросах. Они могут содержать информацию о ресурсе, который нужно получить, или о клиенте, который этот ресурс запрашивает. Примеры: `Accept`, `User-Agent`.
    
3. **Заголовки ответа (Response Headers)**: Отправляются только в HTTP-ответах. Они обычно содержат информацию о сервере и о том, как интерпретировать тело ответа. Примеры: `Server`, `WWW-Authenticate`.
    
4. **Сущностные заголовки (Entity Headers)**: Эти заголовки описывают тело сообщения, его тип и модификации. Примеры: `Content-Type`, `Content-Encoding`.
    

Вот пример отправляемого нашим скриптом заголовка.

Если **httpbin.org** не доступен, используйте

- [http://ipwho.is/](http://ipwho.is/) 
- [https://api.ipify.org?format=json](https://api.ipify.org?format=json)

```
import requests

# Определение заголовков, которые будут отправлены с запросом
headers = {
    'User-Agent': 'Mozilla/5.0',                  # Идентификация типа браузера, который отправляет запрос
    'Accept': 'text/html,application/xhtml+xml',  # Типы контента, которые клиент может обработать
    'Connection': 'keep-alive'                    # Указание на необходимость использования постоянного соединения
}

# Выполнение GET-запроса с установленными заголовками
response = requests.get('http://httpbin.org/user-agent', headers=headers)

print(response.text)


>>> {
       "user-agent": "Mozilla/5.0"
    }
```

Пояснение к каждой строке:

- `headers = {...}`: Создает словарь, содержащий заголовки, которые будут включены в HTTP-запрос.
    
    - `'User-Agent': 'Mozilla/5.0'`: Заголовок `User-Agent` определяет, какой "браузер" используется для запроса. Это может быть полезно, например, для маскировки вашего скрипта под обычный веб-браузер.
        
    - `'Accept': 'text/html,application/xhtml+xml'`: Заголовок `Accept` указывает, какие типы медиа (обычно форматы файлов) клиент может обрабатывать.
        
    - `'Connection': 'keep-alive'`: Заголовок `Connection` с значение `keep-alive` указывает, что соединение с сервером должно оставаться открытым для последующих запросов.
        
- `response = requests.get('http://httpbin.org/user-agent', headers=headers)`: Вызывает метод `.get()` из библиотеки `requests`, передавая URL и заголовки в качестве аргументов. Возвращаемый объект сохраняется в переменной `response`, которая будет содержать всю информацию о полученном ответе, такую как статус-код, заголовки ответа и тело ответа.
    

Давайте сравним `'User-Agent'`с тем что отправляет наш браузер посмотреть его можно введя в поисковую строку браузера команду `chrome://version/`.

```python
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36
```

Для имитации запроса от обычного веб-браузера в программировании на Python часто используется метод `.get()` библиотеки `requests` с именованным атрибутом `headers`. В этот атрибут передается словарь, в котором ключи — это названия HTTP-заголовков, а значения — это соответствующие значения этих заголовков.

### Почему это важно?

Многие веб-сервисы имеют механизмы защиты от автоматического доступа, например, от веб-скраперов или ботов. Один из способов обнаружения такого автоматического доступа — анализ HTTP-заголовков. Отправляя заголовки, которые обычно формируются веб-браузерами, можно "замаскировать" свою программу под обычного пользователя.

```python
import requests  # Импортирование библиотеки для работы с HTTP-запросами

# Создание словаря с заголовками, которые будут использоваться для маскировки под браузер
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br'
}

# Выполнение GET-запроса с указанными заголовками
response = requests.get('http://httpbin.org/user-agent', headers=headers)

# В переменной response хранится ответ сервера, который можно дальше обработать
print(response.text)
```

В этом примере использованы следующие заголовки:

- `User-Agent`: Идентифицирует клиентскую программу, с помощью которой отправляется запрос. В данном случае мы притворяемся браузером Google Chrome версии 91, запущенном на операционной системе Windows 10.
    
- `Accept-Language`: Указывает, какие языки предпочтительны для клиента.
    
- `Accept-Encoding`: Указывает, какие методы сжатия контента клиент может обработать.
    

Таким образом выглядит стандартный питоновский `User-Agent`, это явно указывает на то, что запрос происходит от скрипта, а не от обычного веб-браузера. Это может привести к тому, что ваш IP-адрес будет заблокирован на сервере, особенно если речь идёт о сайтах с хорошо настроенными механизмами защиты от веб-скрапинга.

```
{
  "user-agent": "python-requests/2.28.2"
}
```

Этот `User-Agent` явно говорит серверу, что запрос был сделан с использованием библиотеки `requests` для Python, что часто является признаком автоматизированного доступа.

### Какие меры можно принять?

1. **Смена User-Agent**: Как вы правильно заметили, одним из решений является использование списка различных `User-Agent` из файла и их случайный выбор при каждом запросе, как в вашем примере кода.
    
2. **IP-ротация**: В случае, если сервер блокирует по IP, можно использовать прокси-серверы для ротации IP-адресов.
    
3. **Задержки и ограничение частоты запросов**: Использование `time.sleep()` для введения задержек между запросами может также помочь избежать блокировки.
    
4. **Обработка капчи и других механизмов защиты**: В некоторых случаях может потребоваться распознавание и решение капчи.
    
5. **Реферер и другие заголовки**: Кроме `User-Agent`, можно также настроить другие HTTP-заголовки для имитации поведения настоящего пользователя.
    
6. **Сессии и куки**: Использование сессий и сохранение куки могут сделать ваш скрапинг менее подозрительным.
    
7. **Анализ JavaScript**: Некоторые сайты требуют выполнения JavaScript для доступа к контенту. В этом случае можно использовать библиотеки, которые могут интерпретировать JavaScript.
    

Загрузите файл с набором фейковых **user-agent** и поместите его в корневой каталог с вашим проектом. [Скачать файл](https://drive.google.com/file/d/1mIG_570jp_NSlPgeyCF2xOOZLjP2V82w/view?usp=sharing)

```python
import requests
from random import choice

url = 'http://httpbin.org/user-agent'

with open('user_agent.txt') as file:
    lines = file.read().split('\n')

for line in lines:
    user_agent = {'user-agent': choice(lines)}
    response = requests.get(url=url, headers=user_agent)
    print(response.text)


>>> {
  "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 YaBrowser/21.2.4.165 Yowser/2.5 Safari/537.36"
}

{
  "user-agent": "Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36"
}
.....
```

- **Случайный выбор User-Agent**: Это полезно, если вам нужно имитировать поведение различных браузеров или если сервер применяет ограничения к слишком частым запросам с одинаковыми `User-Agent` за короткий промежуток.
    
- **Задержки между запросами**: В этом коде не реализованы задержки между запросами. В реальных задачах это может быть полезно для избегания блокировки со стороны сервера. Это можно сделать с помощью `time.sleep(randint(1,5))`, как вы упомянули.
    

### Использование в различных сценариях

- **Простой парсер**: Если вы пишете простой парсер, то первый пример (с фиксированным `User-Agent`) будет достаточен в большинстве случаев.
    
- **Высокочастотный парсер или обход блокировок**: Второй пример (со случайным выбором `User-Agent` и возможными задержками) будет более подходящим, если сервер имеет механизмы защиты от автоматического доступа или если необходимо собирать данные с высокой частотой.


#ПарсингPython 