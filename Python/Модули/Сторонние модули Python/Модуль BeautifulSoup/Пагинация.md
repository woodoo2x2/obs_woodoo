Пагинация - это ключевой элемент веб-дизайна, который не только обеспечивает удобство навигации, но и позволяет нам, разработчикам и аналитикам, быстро определить общее количество доступных страниц.

Пагинация (англ. [Pagination](https://en.wikipedia.org/wiki/Pagination)) встречается повсюду, и нам приходится сталкиваться с ней почти всегда при написании парсера. В этом разделе мы рассмотрим пагинацию, научимся проходить по ней в цикле и извлекать информацию с каждой страницы.

В нашем [тренажере](http://parsinger.ru/legal/html/index1_page_1.html) пагинация также присутствует. На скриншоте выделены две области. В верхней области представлена ссылка, которая заканчивается числом. Как правило, это число соответствует номеру страницы. Мы можем модифицировать эту ссылку, выполняя запросы на каждой итерации. Во второй области представлены кнопки пагинации.

В нашем тренажере используется простая пагинация, включающая всего 4 страницы, но этого вполне достаточно для понимания механизма работы.

![](https://ucarecdn.com/7be27ac9-a7e5-4c5c-bf58-fba1304e0957/)

Итак, приступим к задаче. Первым делом, нам нужно определить, сколько всего страниц представлено на сайте. Эта информация обычно содержится в блоке пагинации, который позволяет пользователю переключаться между разными страницами контента.

Чтобы понять, как работает пагинация и как из неё извлечь нужную информацию, давайте рассмотрим HTML-код этого блока. Особое внимание стоит уделить структуре этого кода, а также атрибутам, которые могут нам помочь в дальнейшем.

![](https://ucarecdn.com/1f30ecbd-795e-46c7-b987-5cbefcc6689f/)

В данном случае мы видим, что пагинация реализована с помощью блока `<div>` с классом `'pagen'`. Внутри этого блока находятся четыре ссылки, представленные тегами `<a>`.

Каждая из этих ссылок ведет на определенную страницу контента. Для извлечения этих ссылок мы можем использовать методы `.find()` и `.find_all()`, с которыми мы уже знакомы из предыдущих уроков. Эти методы позволят нам легко найти и собрать все необходимые нам ссылки из блока пагинации.

Кроме того, в контексте пагинации часто важно знать, какая из страниц является последней. В нашем случае это значение последнего элемента. Зная номер последней страницы, мы можем определить, сколько итераций нам потребуется для обработки всего контента, и соответственно настроить наш парсер на работу с каждой из страниц.

```python
from bs4 import BeautifulSoup
import requests

# Задаем URL-адрес веб-страницы для парсинга
url = 'http://parsinger.ru/html/index1_page_3.html'

# Отправляем GET-запрос к указанной странице
response = requests.get(url=url)

# Устанавливаем кодировку ответа сервера в UTF-8 для корректного отображения текста на кириллице
response.encoding = 'utf-8'

# Преобразуем текст ответа сервера в объект BeautifulSoup с использованием парсера 'lxml'
soup = BeautifulSoup(response.text, 'lxml')

# Ищем блок пагинации (элемент <div> с классом 'pagen') на странице, 
# затем извлекаем из него все вложенные ссылки (элементы <a>)
pagen = soup.find('div', class_='pagen').find_all('a')

# Выводим на экран список найденных ссылок
print(pagen)
```

Вывод:

```
[<a href="index1_page_1.html">1</a>, <a href="index1_page_2.html">2</a>, <a href="index1_page_3.html">3</a>, <a href="index1_page_4.html">4</a>]
```

В результате выполнения кода мы получили список HTML-тегов `<a>`. Однако наша цель — извлечь из них атрибут `href`, который содержит ссылку, а также текстовое содержимое каждого тега.

Для более компактного и эффективного решения этой задачи можно использовать механизм list comprehension. Это позволяет создавать списки на лету, применяя к каждому элементу определенное выражение.

Пример использования list comprehension:

```
pagen = [link['href'] for link in soup.find('div', class_='pagen').find_all('a')]
```

В результате выполнения этого кода мы получим следующий список ссылок:

```
['index1_page_1.html', 'index1_page_2.html', 'index1_page_3.html', 'index1_page_4.html']
```

Если вы не знакомы с list comprehension, можно представить его аналог в виде обычного цикла, результат будет тот же:

```
pagen = []
for link in soup.find('div', class_='pagen').find_all('a'):
    pagen.append(link['href'])
```

В процессе веб-скрапинга очень важно уметь извлекать различные атрибуты из HTML-тегов. Например, атрибут `href` у тега `<a>` указывает на URL-адрес, на который ведет данная ссылка. По аналогии, у тега `<img>` есть атрибут `src`, который содержит URL-адрес изображения.

Однако просто извлечение этих атрибутов не всегда дает нам полноценные ссылки. Во многих случаях, особенно при работе с пагинацией, мы получаем относительные пути к файлам или изображениям. Это значит, что перед тем, как использовать их, нам нужно преобразовать их в полные URL-адреса.

В Python для этого можно использовать f-строки — это способ форматирования строк, который позволяет вставлять значения переменных прямо в текст. Например, если у нас есть базовый URL-адрес и относительный путь, мы можем объединить их следующим образом:

За схемой далеко ходить не нужно, стоит обратить внимание на ссылку.

[**https://parsinger.ru/html/**index1_page_1.html](https://parsinger.ru/html/index1_page_1.html)

Виды путей.

```
base_url = 'http://example.com/'
relative_path = 'path/to/file.html'
full_url = f'{base_url}{relative_path}'
```

Теперь, когда мы понимаем, как формируются URL-адреса на сайте, мы можем разработать схему для их генерации. Основная идея заключается в том, чтобы определить общую структуру URL-адресов и использовать ее для создания полных ссылок на нужные нам страницы или ресурсы.

Чтобы понять эту структуру, нам не нужно искать дополнительную информацию в других источниках. Все, что нам нужно, уже есть в адресной строке браузера. Анализируя ее, мы можем выявить закономерности и использовать их для создания наших собственных ссылок.

Создадим переменную `schema` и сохраним в нее первую часть ссылки. И в цикле на каждой итерации мы будем склеивать обе части, чтобы получить корректную ссылку.

```python
from bs4 import BeautifulSoup
import requests

# Задаем URL-адрес веб-страницы для парсинга
url = 'http://parsinger.ru/html/index1_page_3.html'

# Отправляем GET-запрос к указанной странице
response = requests.get(url=url)

# Устанавливаем кодировку ответа сервера в UTF-8 для корректного отображения текста на кириллице
response.encoding = 'utf-8'

# Преобразуем текст ответа сервера в объект BeautifulSoup с использованием парсера 'lxml'
soup = BeautifulSoup(response.text, 'lxml')

# Ищем блок пагинации и извлекаем все вложенные ссылки
pagen = soup.find('div', class_='pagen').find_all('a')

# Инициализируем список для хранения абсолютных URL-адресов
list_link = []

# Задаем схему URL-адреса, которая будет использоваться для преобразования относительных путей в абсолютные URL
schema = 'http://parsinger.ru/html/'

# Цикл по всем найденным ссылкам для преобразования их в абсолютные URL-адреса
for link in pagen:
    list_link.append(f"{schema}{link['href']}")

# Выводим на экран список абсолютных URL-адресов
print(list_link)
```

Вывод:

```
['http://parsinger.ru/html/index1_page_1.html', 'http://parsinger.ru/html/index1_page_2.html', 'http://parsinger.ru/html/index1_page_3.html', 'http://parsinger.ru/html/index1_page_4.html']
```

Отлично, всё получилось. 

То же самое, но с применением **list comprehension:**

```
from bs4 import BeautifulSoup
import requests

url = 'http://parsinger.ru/html/index1_page_3.html'
response = requests.get(url=url)
response.encoding = 'utf-8'
soup = BeautifulSoup(response.text, 'lxml')
schema = 'http://parsinger.ru/html/'
pagen = [f"{schema}{link['href']}" for link in soup.find('div', class_='pagen').find_all('a')]

print(pagen)
```

Вывод:

```
['http://parsinger.ru/html/index1_page_1.html', 'http://parsinger.ru/html/index1_page_2.html', 'http://parsinger.ru/html/index1_page_3.html', 'http://parsinger.ru/html/index1_page_4.html']
```

Иногда на сайтах все ссылки на страницы (как кнопки "вперед" или "назад" в интернет-магазинах) не показываются сразу. Это как книга, где оглавление показывает только часть глав.

Допустим, вы хотите собрать информацию со всех страниц сайта, но у вас есть ссылка только на одну или несколько из них. Что делать? Нужно самому "собрать" эти ссылки.

Посмотрим на пример с сайтом www.ozon.ru. Если вы присмотрите к адресам страниц в интернет-магазинах, то заметите, что они часто похожи между собой, и отличаются, например, номером в конце. Этот номер и показывает, какая это страница по счету.

`https://www.ozon.ru/category/knigi-16500/?category_was_predicted=true&**page=2**`

Зная это, вы можете написать код, который сам будет генерировать ссылки на все страницы, даже если на сайте они не отображаются. Это как если бы ты знал, что после главы 1 идет глава 2, потом 3 и так далее, и просто листал книгу дальше.

Таким образом, даже если сайт не показывает все ссылки сразу, вы все равно сможете получить информацию со всех страниц.

Мы можем использовать эту особенность, чтобы сгенерировать себе новые ссылки на все страницы.

Пример: если у вас есть ссылка `http://сайт.ру/страница_1`, то следующая страница, скорее всего, будет `http://сайт.ру/страница_2`, потом `страница_3` и так далее.

В Python есть удобная штука, которая называется [f-строка](https://shultais.education/blog/python-f-strings). Это способ создавать строки, вставляя в них значения переменных. Просто представьте, что это как конструктор: вы берёте основную часть строки и добавляешь к ней то, что нужно изменить (в нашем случае — номер страницы).

```
link = []
for i in range(1, 101):
    link.append(f'https://www.ozon.ru/category/knigi-16500/?category_was_predicted=true&page={i}')
print(link)
```

Вывод:

```
['https://www.ozon.ru/category/knigi-16500/?category_was_predicted=true&page=1', 
 'https://www.ozon.ru/category/knigi-16500/?category_was_predicted=true&page=2', 
    ...
 'https://www.ozon.ru/category/knigi-16500/?category_was_predicted=true&page=100']
```

Таким способом мы можем сразу создать 100 ссылок для парсинга. Но проблема в том, что мы заранее не можем быть уверены в количестве страниц на сайте. Сегодня их может быть 100, а завтра — 105 или 95. Если мы не учтем это, наш парсер может столкнуться с ошибкой или пропустить новые страницы.

По этой причине перед созданием ссылок нам стоит узнать, какое последнее значение находится в пагинации.

За основу примера возьмём наш [тренажер](http://parsinger.ru/html/index1_page_1.html).

```
from bs4 import BeautifulSoup
import requests

url = 'http://parsinger.ru/html/index1_page_3.html'
response = requests.get(url=url)
response.encoding = 'utf-8'
soup = BeautifulSoup(response.text, 'lxml')
schema = 'http://parsinger.ru/html/'
pagen = [link.text for link in soup.find('div', class_='pagen').find_all('a')][-1]

print(pagen)

>>> 4
```

Мы применили индексацию **[-1]**, чтобы получить последний элемент списка, в котором хранился весь список значений пагинации.

Выполните код выше у себя в терминале, посмотрите на результат, без метода `.text` и без индекса. Попытайтесь понять, почему мы используем этот подход. 

![](https://ucarecdn.com/b01ebed8-74b9-4f86-a90e-cfae1fb04ffa/)

Если мы не узнаем последнее значение в пагинации, то наша задача — проверять каждую страницу на наличие контента. Другими словами, мы будем генерировать ссылки и проверять их. Если сервер отвечает кодом 200 (что значит "Всё хорошо, страница найдена"), мы продолжаем парсинг собирая нужную информацию. Если же ответ другой (например, 404 — "Страница не найдена"), значит, такой страницы нет, и нам пора остановиться.

Давайте взглянем на код:

```python
# Наглядный псевокод

from bs4 import BeautifulSoup
import requests

base_url = "http://сайт.ру/страница_"
num_page = 1  # начнем с первой страницы

while True:
    url = f"{base_url}{num_page }"
    response = requests.get(url)
    
    # Если статус ответа 200, продолжаем парсинг
    if ответ.status_code == 200:
        # Здесь ваш код для парсинга содержимого страницы
        # ...
        num_page += 1
    else:
        # Если статус ответа не 200, завершаем цикл
        break
```

Этот код будет переходить на следующую страницу, пока не встретит страницу, которой нет (или другую ошибку). Таким образом, он автоматически определит, сколько страниц нужно обработать.

**Почему пагинация важна для веб-парсинга?**

При парсинге важно уметь обрабатывать пагинацию, так как это позволит извлекать данные со всех страниц, а не только с одной.

**Основные типы пагинации:**

- **Традиционная пагинация:** Когда страницы разбиты по номерам (например, 1, 2, 3…), и пользователь может кликнуть по номеру для перехода.
    
- **Бесконечная прокрутка:** Когда новый контент загружается автоматически при прокрутке вниз.
    
    - Основная идея заключается в том, чтобы выяснить, откуда берется информация для новой порции данных при прокрутке. Часто это делается через AJAX или API запросы. После определения запроса вы можете имитировать его с помощью `requests`.
        
- **"Показать еще" или "Загрузить еще":** Пользователь должен кликнуть на кнопку, чтобы загрузить больше контента.
    
    - Зачастую, при нажатии на такую кнопку, отправляется AJAX-запрос, который можно имитировать аналогично бесконечной прокрутке.
        

### **Рекомендации:**

- Всегда обращайте внимание на URL при переходе между страницами пагинации. Это может дать подсказку о структуре URL для различных страниц.
    
- Иногда полезно использовать инструменты разработчика в вашем браузере для мониторинга сетевых запросов при переходе между страницами или при прокрутке.
    
- Будьте внимательны к ограничениям сайта. Некоторые сайты могут блокировать вас за слишком частые запросы.
    

## **Особенности, нюансы и потенциальные проблемы**

**1. Особенности пагинации:**

- **Динамическая vs. Статическая**: Некоторые сайты используют статическую пагинацию (URL меняется при переходе на другую страницу), в то время как другие применяют динамическую пагинацию (контент подгружается без изменения URL).
    
- **Задержка загрузки**: Некоторые сайты могут задерживать загрузку следующей порции контента, чтобы предотвратить быстрое автоматическое скачивание данных.
    

**2. Нюансы:**

- **Изменение структуры URL**: На одних сайтах номер страницы может идти в конце URL (`/page/2`), на других - как параметр (`?page=2`), а на третьих - вовсе как часть API запроса.
    
- **Кнопки vs. Прокрутка**: В то время как некоторые сайты предоставляют явные кнопки для перехода между страницами, другие используют автоматическую прокрутку, которая может быть сложнее для имитации при парсинге.
    

**3. Потенциальные проблемы:**

- **Ограничение по скорости**: Если вы делаете слишком много запросов за короткий промежуток времени, сайт может временно или постоянно заблокировать ваш IP.
    
- **CAPTCHA**: Для борьбы с парсингом некоторые сайты могут предложить вам решить [CAPTCHA](https://stepik.org/lesson/801962/step/2?unit=804937).
    
- **Бесконечная пагинация**: Определение того, когда заканчиваются данные на сайтах с бесконечной прокруткой, может быть сложной задачей.
    
- **Динамические элементы**: Сайты, использующие фреймворки вроде React или Angular, могут динамически загружать контент, что затрудняет его извлечение с помощью традиционных методов парсинга.
    

**4. Рекомендации:**

- **Использование прокси**: Чтобы обойти ограничения по скорости и блокировки, рассмотрите возможность использования прокси-серверов.
    
- **Соблюдение `robots.txt`**: Перед началом парсинга проверьте файл `robots.txt` на сайте, чтобы удостовериться, что вы имеете право парсить нужные вам страницы.
    
- **Имитация действий пользователя**: Используйте инструменты, такие как Selenium, чтобы имитировать действия реального пользователя, если это необходимо для обхода динамической пагинации или других сложных элементов.

#ПарсингPython 